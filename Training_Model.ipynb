{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Emotion Recognition Model</h2>\n",
        "<h4>by A Bhagyalaxmi</h4>\n",
        "Using <i>Convolutional Neural Network</i> creating an Emotion Detection Web Application which can classify the emotion of a person into 7 different categories: Happy, Sad, Neutral, Angry, Fear, Surprise and Disgust.\n",
        "\n",
        "Dataset: FER2013\n",
        "\n",
        "Tech stack: TensorFlow, OpenCV, numpy, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Prerequisties</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wvGxjjeV-9Ls",
        "outputId": "7527e2eb-2724-446d-9a6c-c516f00c7e41",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow version: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import utils\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from IPython.display import SVG, Image\n",
        "import tensorflow as tf\n",
        "from livelossplot import PlotLossesKeras\n",
        "print(\"Tensorflow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Normalising the dataset</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 25725 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ],
      "source": [
        "train_dir = 'train'\n",
        "val_dir = 'test'\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=64,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=64,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Using Convolutional Neural Network model to train the dataset to classifying the emotions.</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "emotion_model = Sequential()\n",
        "\n",
        "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1))) #Input image of dimension 48Ã—48\n",
        "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "emotion_model.add(Dropout(0.25))\n",
        "\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "emotion_model.add(Dropout(0.25))\n",
        "emotion_model.add(Flatten())\n",
        "\n",
        "emotion_model.add(Dense(1024, activation='relu'))\n",
        "emotion_model.add(Dropout(0.5))\n",
        "\n",
        "emotion_model.add(Dense(7, activation='softmax')) # Output layer containing of 7 different emotions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Compile the model using accuracy to measure model performance.</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "350/350 [==============================] - 229s 653ms/step - loss: 1.6994 - accuracy: 0.2732 - val_loss: 1.9172 - val_accuracy: 0.2497\n",
            "Epoch 2/50\n",
            "350/350 [==============================] - 93s 266ms/step - loss: 1.6259 - accuracy: 0.3255 - val_loss: 1.8873 - val_accuracy: 0.3468\n",
            "Epoch 3/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 1.5274 - accuracy: 0.3938 - val_loss: 1.8493 - val_accuracy: 0.3838\n",
            "Epoch 4/50\n",
            "350/350 [==============================] - 91s 261ms/step - loss: 1.4536 - accuracy: 0.4306 - val_loss: 1.7300 - val_accuracy: 0.4032\n",
            "Epoch 5/50\n",
            "350/350 [==============================] - 91s 259ms/step - loss: 1.4039 - accuracy: 0.4518 - val_loss: 1.6932 - val_accuracy: 0.4223\n",
            "Epoch 6/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 1.3622 - accuracy: 0.4686 - val_loss: 1.6320 - val_accuracy: 0.4319\n",
            "Epoch 7/50\n",
            "350/350 [==============================] - 90s 257ms/step - loss: 1.3127 - accuracy: 0.4860 - val_loss: 1.6295 - val_accuracy: 0.4350\n",
            "Epoch 8/50\n",
            "350/350 [==============================] - 90s 257ms/step - loss: 1.2922 - accuracy: 0.4982 - val_loss: 1.6000 - val_accuracy: 0.4508\n",
            "Epoch 9/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 1.2605 - accuracy: 0.5088 - val_loss: 1.5174 - val_accuracy: 0.4615\n",
            "Epoch 10/50\n",
            "350/350 [==============================] - 90s 257ms/step - loss: 1.2328 - accuracy: 0.5227 - val_loss: 1.5419 - val_accuracy: 0.4722\n",
            "Epoch 11/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 1.2111 - accuracy: 0.5299 - val_loss: 1.5376 - val_accuracy: 0.4633\n",
            "Epoch 12/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 1.1789 - accuracy: 0.5448 - val_loss: 1.4983 - val_accuracy: 0.4794\n",
            "Epoch 13/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 1.1622 - accuracy: 0.5535 - val_loss: 1.4321 - val_accuracy: 0.4838\n",
            "Epoch 14/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 1.1409 - accuracy: 0.5598 - val_loss: 1.4323 - val_accuracy: 0.4950\n",
            "Epoch 15/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 1.1159 - accuracy: 0.5746 - val_loss: 1.4188 - val_accuracy: 0.4954\n",
            "Epoch 16/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 1.0994 - accuracy: 0.5795 - val_loss: 1.3519 - val_accuracy: 0.5074\n",
            "Epoch 17/50\n",
            "350/350 [==============================] - 91s 259ms/step - loss: 1.0761 - accuracy: 0.5885 - val_loss: 1.4392 - val_accuracy: 0.5010\n",
            "Epoch 18/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 1.0543 - accuracy: 0.6017 - val_loss: 1.3515 - val_accuracy: 0.5123\n",
            "Epoch 19/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 1.0410 - accuracy: 0.6043 - val_loss: 1.3286 - val_accuracy: 0.5199\n",
            "Epoch 20/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 1.0261 - accuracy: 0.6108 - val_loss: 1.3786 - val_accuracy: 0.5188\n",
            "Epoch 21/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 1.0040 - accuracy: 0.6220 - val_loss: 1.2833 - val_accuracy: 0.5325\n",
            "Epoch 22/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.9879 - accuracy: 0.6261 - val_loss: 1.3053 - val_accuracy: 0.5239\n",
            "Epoch 23/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.9786 - accuracy: 0.6305 - val_loss: 1.3311 - val_accuracy: 0.5262\n",
            "Epoch 24/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.9553 - accuracy: 0.6362 - val_loss: 1.2793 - val_accuracy: 0.5437\n",
            "Epoch 25/50\n",
            "350/350 [==============================] - 91s 259ms/step - loss: 0.9312 - accuracy: 0.6502 - val_loss: 1.3162 - val_accuracy: 0.5393\n",
            "Epoch 26/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.9198 - accuracy: 0.6544 - val_loss: 1.2622 - val_accuracy: 0.5448\n",
            "Epoch 27/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.9013 - accuracy: 0.6596 - val_loss: 1.3205 - val_accuracy: 0.5459\n",
            "Epoch 28/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.8873 - accuracy: 0.6658 - val_loss: 1.2870 - val_accuracy: 0.5467\n",
            "Epoch 29/50\n",
            "350/350 [==============================] - 91s 259ms/step - loss: 0.8668 - accuracy: 0.6724 - val_loss: 1.2610 - val_accuracy: 0.5527\n",
            "Epoch 30/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.8478 - accuracy: 0.6845 - val_loss: 1.3126 - val_accuracy: 0.5537\n",
            "Epoch 31/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.8345 - accuracy: 0.6889 - val_loss: 1.3180 - val_accuracy: 0.5516\n",
            "Epoch 32/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.8202 - accuracy: 0.6928 - val_loss: 1.3346 - val_accuracy: 0.5484\n",
            "Epoch 33/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.7978 - accuracy: 0.7056 - val_loss: 1.3101 - val_accuracy: 0.5525\n",
            "Epoch 34/50\n",
            "350/350 [==============================] - 91s 259ms/step - loss: 0.7811 - accuracy: 0.7113 - val_loss: 1.2500 - val_accuracy: 0.5640\n",
            "Epoch 35/50\n",
            "350/350 [==============================] - 91s 259ms/step - loss: 0.7629 - accuracy: 0.7164 - val_loss: 1.2808 - val_accuracy: 0.5608\n",
            "Epoch 36/50\n",
            "350/350 [==============================] - 91s 259ms/step - loss: 0.7486 - accuracy: 0.7222 - val_loss: 1.3373 - val_accuracy: 0.5592\n",
            "Epoch 37/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.7355 - accuracy: 0.7280 - val_loss: 1.2476 - val_accuracy: 0.5691\n",
            "Epoch 38/50\n",
            "350/350 [==============================] - 90s 257ms/step - loss: 0.7076 - accuracy: 0.7378 - val_loss: 1.3081 - val_accuracy: 0.5647\n",
            "Epoch 39/50\n",
            "350/350 [==============================] - 91s 261ms/step - loss: 0.7068 - accuracy: 0.7368 - val_loss: 1.3263 - val_accuracy: 0.5615\n",
            "Epoch 40/50\n",
            "350/350 [==============================] - 91s 259ms/step - loss: 0.6811 - accuracy: 0.7474 - val_loss: 1.3358 - val_accuracy: 0.5638\n",
            "Epoch 41/50\n",
            "350/350 [==============================] - 91s 259ms/step - loss: 0.6628 - accuracy: 0.7567 - val_loss: 1.3331 - val_accuracy: 0.5657\n",
            "Epoch 42/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.6441 - accuracy: 0.7641 - val_loss: 1.3278 - val_accuracy: 0.5709\n",
            "Epoch 43/50\n",
            "350/350 [==============================] - 91s 259ms/step - loss: 0.6305 - accuracy: 0.7689 - val_loss: 1.3183 - val_accuracy: 0.5695\n",
            "Epoch 44/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.6188 - accuracy: 0.7717 - val_loss: 1.3302 - val_accuracy: 0.5714\n",
            "Epoch 45/50\n",
            "350/350 [==============================] - 91s 259ms/step - loss: 0.5945 - accuracy: 0.7830 - val_loss: 1.3928 - val_accuracy: 0.5657\n",
            "Epoch 46/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.5813 - accuracy: 0.7885 - val_loss: 1.3419 - val_accuracy: 0.5738\n",
            "Epoch 47/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.5699 - accuracy: 0.7887 - val_loss: 1.3487 - val_accuracy: 0.5691\n",
            "Epoch 48/50\n",
            "350/350 [==============================] - 90s 258ms/step - loss: 0.5550 - accuracy: 0.7950 - val_loss: 1.3665 - val_accuracy: 0.5709\n",
            "Epoch 49/50\n",
            "350/350 [==============================] - 91s 259ms/step - loss: 0.5378 - accuracy: 0.8014 - val_loss: 1.3882 - val_accuracy: 0.5762\n",
            "Epoch 50/50\n",
            "350/350 [==============================] - 91s 259ms/step - loss: 0.5243 - accuracy: 0.8092 - val_loss: 1.3470 - val_accuracy: 0.5780\n"
          ]
        }
      ],
      "source": [
        "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
        "emotion_model_info = emotion_model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=22400 // 64,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=7178 // 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the model as h5 file.\n",
        "emotion_model.save('model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JSON file for transmitting data between the web application and the model\n",
        "model_json = emotion_model.to_json()\n",
        "with open(\"model_try.json\",\"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Facial_Expression_Training.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
